\documentclass{article}
\usepackage{hyperref}
\usepackage{amsmath}

\title{Partial least squares}
\author{John Reid}

\begin{document}

\section{Partial least squares model}

The partial least
squares model is presented as a latent variable model in Kevin Murphy's 2012
book \href{https://probml.github.io/pml-book/}{Machine Learning: A
Probabilistic Perspective}.

\begin{eqnarray}
  p(z) &=& \mathcal{N}(z|0, I) \\
  p(v|z, W, \mu, \sigma) &=& \mathcal{N}(v|W z + \mu, \sigma^2 I)
\end{eqnarray}
where
\begin{eqnarray}
  W &=& \begin{pmatrix}
      W_y & 0 \\
      W_x & B_x \\
    \end{pmatrix} \\
  z &=& (z^s; z^x) \\
  v &=& (y; x) \\
  \mu &=& (\mu_y; \mu_x).
\end{eqnarray}
Marginalising $z$ gives
\begin{eqnarray}
  p(v|W, \mu, \sigma)
  &=& \int \mathcal{N}(v|W z + \mu, \sigma^2 I) \mathcal{N}(z|0, I)\,dz \\
  \label{eqn:joint_xy}
  &=& \mathcal{N}(v|\mu, W W^T + \sigma^2 I)
\end{eqnarray}
Conditioning on $x$ gives
\begin{eqnarray}
  p(y|x) &=& \mathcal{N}(y|m_{y|x}, S_{y|x})
\end{eqnarray}
where
\begin{eqnarray}
  C &=& {(B_x B_x^T + W_x W_x^T + \sigma^2 I)}^{-1} \\
  m_{y|x} &=& \mu_y + W_y W_x^T C (x - \mu_x) \\
  S_{y|x} &=& \sigma^2 I + W_y W_y^T - W_y W_x^T C W_x W_y^T
\end{eqnarray}

Suppose we now obtain $N$ independent observations from the model
\begin{eqnarray}
  v_n = (y_n; x_n),\quad 1 \le n \le N.
\end{eqnarray}
We wish to estimate $W, \mu \text{ and } \sigma$. We can do this by maximising the likelihood of the data
$v = x, y$~(\ref{eqn:joint_xy}) using stochastic gradient descent.

One way to validate that our implementation works is to sample data from the model and compare the estimated
parameters to those that we used to sample with. Suppose we sample $v_n$ from our model parameterised by
$\sigma^*, \mu^*_x, \mu^*_y, W^*_y, W^*_x, B^*_x$.
We can estimate $\hat{\sigma}, \hat{\mu}_x, \hat{\mu}_y, \hat{W}_y, \hat{W}_x, \hat{B}_x$ that maximise
\begin{eqnarray}
  \label{eqn:log_joint}
  \sum_n \log p(v|\sigma, \mu, W_y, W_x, B_x)
\end{eqnarray}
We would love to compare the estimated parameters to the underlying parameters but unfortunately (\ref{eqn:log_joint})
is invariant to orthonormal transformations of the $W_y, W_x, B_x$. That is
\begin{eqnarray}
  \sum_n \log p(v_n|\sigma, \mu, W_y U_s, W_x U_s, B_x U_x)
  &=&
  \sum_n \log p(v_n|\sigma, \mu, W_y, W_x, B_x)
\end{eqnarray}
for any orthonormal square matrices (of suitable dimension) $U_s, U_x$. We can see this as $W$ only appears
as $W W^T$ in the likelihood and $W W^T = W U U^T W^T$ for any orthonormal $U$.
\end{document}
