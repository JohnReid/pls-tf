#!/usr/bin/env python

"""Implementation of partial least squares model (section 12.5.2 in Kevin Murphy's PML 2012 book)."""

from pathlib import Path
import math
import datetime
import tensorflow as tf
from tensorflow import keras
import tensorflow_probability as tfp
import matplotlib.pyplot as plt
from matplotlib import colors
import seaborn as sns
plt.ion()
plt.close('all')
la = tf.linalg
tf.__version__
tfp.__version__
tf.random.set_seed(37)


# Dimensions
dx = 19   # Dimension of xs
dy = 5    # Dimension of ys
dzs = 5   # Dimension of shared z
dzx = 14  # Dimension of z for xs
N = 100
sigma_true = .3

# Sample ground truth
std_normal = tfp.distributions.Normal(loc=0, scale=1.)
zx_true = std_normal.sample([N, dzx])
zs_true = std_normal.sample([N, dzs])
z_true = tf.concat([zx_true, zs_true], axis=1)
mu_y_true = std_normal.sample([dy])
mu_x_true = std_normal.sample([dx])
Wy_true = std_normal.sample([dy, dzs]) / math.sqrt(dzs)
Wx_true = std_normal.sample([dx, dzs]) / math.sqrt(dzs) / math.sqrt(2)
Bx_true = std_normal.sample([dx, dzx]) / math.sqrt(dzx) / math.sqrt(2)
p_y_given_z = tfp.distributions.MultivariateNormalDiag(loc=mu_y_true
                                                           + la.matvec(Wy_true, zs_true),
                                                       scale_identity_multiplier=sigma_true)
y_true = p_y_given_z.sample()
p_x_given_z = tfp.distributions.MultivariateNormalDiag(loc=mu_x_true
                                                           + la.matvec(Wx_true, zs_true)
                                                           + la.matvec(Bx_true, zx_true),
                                                       scale_identity_multiplier=sigma_true)
x_true = p_x_given_z.sample()

# Check distribution of ground truth
tf.math.reduce_std(z_true, axis=0)
tf.math.reduce_std(y_true, axis=0)
tf.math.reduce_std(x_true, axis=0)

# Cross-covariance of X and Y
Xbar = x_true / tf.reduce_mean(x_true, axis=0)
Ybar = y_true / tf.reduce_mean(y_true, axis=0)
XTY = tf.matmul(Xbar, Ybar, transpose_a=True)
s, u, v = la.svd(XTY)
tf.reduce_max(tf.abs(XTY - tf.matmul(u, tf.matmul(tf.linalg.diag(s), v, adjoint_b=True))))
s.shape
u.shape
v.shape

# Parameterise
#
# Noise
# sigma = tf.Variable(tf.abs(std_normal.sample([1])), name='sigma')
sigma = tf.Variable([sigma_true], name='sigma')
# Data means
mu_x = tf.Variable(tf.reduce_mean(x_true, axis=0))
mu_y = tf.Variable(tf.reduce_mean(y_true, axis=0))
# Maps from latent spaces
Bx = tf.Variable(std_normal.sample([dx, dzx]))
Wx = tf.Variable(std_normal.sample([dx, dzs]))
Wy = tf.Variable(std_normal.sample([dy, dzs]))
# initialise based on cross-covariance
# Wx = Wx[:, :min(dx, dy)].assign(u * tf.sqrt(s))
# Wy = Wy[:, :min(dx, dy)].assign(v * tf.sqrt(s))
# Keep all parameters together
trainable_params = [
    # sigma,
    Bx,
    Wx,
    Wy,
    mu_x,
    mu_y]

# Training loop
#
# Instantiate an optimizer.
initial_learning_rate = 1e-3
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=100,
    decay_rate=0.96,
    staircase=True)

# Metrics
# train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)
# train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')

current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
log_dir = Path('logs') / current_time
summary_writer = tf.summary.create_file_writer(str(log_dir))

optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)
iterations = 50
for iteration in range(iterations):
    # Open a GradientTape to record the operations run
    # during the forward pass, which enables auto-differentiation.
    with tf.GradientTape() as tape:

        # Run the forward pass of the layer.
        # The operations that the layer applies
        # to its inputs are going to be recorded
        # on the GradientTape.

        sigma2_Ix = la.diag(sigma**2 * tf.ones(dx))
        sigma2_Iy = la.diag(sigma**2 * tf.ones(dy))
        # Useful products
        BxBxT = tf.matmul(Bx, Bx, transpose_b=True)
        WxWxT = tf.matmul(Wx, Wx, transpose_b=True)
        WxWyT = tf.matmul(Wx, Wy, transpose_b=True)
        WyWyT = tf.matmul(Wy, Wy, transpose_b=True)
        # Calculate terms for posterior mean and covariance
        BxBxT_WxWxT_chol = la.cholesky(BxBxT + WxWxT + sigma2_Ix)
        WxWyT_term = la.triangular_solve(BxBxT_WxWxT_chol, WxWyT)
        x_mux_term = tf.squeeze(la.triangular_solve(BxBxT_WxWxT_chol, tf.expand_dims(x_true - mu_x, axis=-1)), axis=-1)
        # Create posterior
        likelihood_loc = mu_y + la.matvec(WxWyT_term, x_mux_term, transpose_a=True)
        likelihood_cov = WyWyT + sigma2_Iy - la.matmul(WxWyT_term, WxWyT_term, transpose_a=True)
        p_y_given_x = tfp.distributions.MultivariateNormalTriL(loc=likelihood_loc,
                                                            scale_tril=la.cholesky(likelihood_cov))
        # Likelihood of y
        lp = p_y_given_x.log_prob(y_true)
        lp_sum = tf.reduce_sum(lp)

        # Parameter priors
        sigma_prior = tf.squeeze(tfp.distributions.LogNormal(loc=0, scale=.25).log_prob(sigma))

        # Loss
        joint = lp_sum + sigma_prior
        loss = - joint

    # Use the gradient tape to automatically retrieve
    # the gradients of the trainable variables with respect to the loss.
    grads = tape.gradient(loss, trainable_params)

    # Run one step of gradient descent by updating
    # the value of the variables to minimize the loss.
    optimizer.apply_gradients(zip(grads, trainable_params))

    print("Training joint probability at iteration %d: %.4f" % (iteration, float(joint)))
    with summary_writer.as_default():
        tf.summary.scalar('joint/total', joint, step=iteration)
        tf.summary.scalar('joint/lp_sum', lp_sum, step=iteration)
        tf.summary.scalar('joint/sigma_prior', sigma_prior, step=iteration)

# Examine matrix parameters
divnorm = colors.TwoSlopeNorm(vmin=-5., vcenter=0., vmax=10)
fig, axes = plt.subplots(ncols=3, figsize=(12, 4))
sns.heatmap(tf.matmul(Bx_true, Bx, transpose_a=True), ax=axes[0], cmap='PuOr', norm=divnorm)
sns.heatmap(tf.matmul(Wx_true, Wx, transpose_a=True), ax=axes[1], cmap='PuOr', norm=divnorm)
sns.heatmap(tf.matmul(Wy_true, Wy, transpose_a=True), ax=axes[2], cmap='PuOr', norm=divnorm)
axes[0].set_title('B_x')
axes[1].set_title('W_x')
axes[2].set_title('W_y')
plt.show()

# Examine matrix parameters
divnorm=colors.TwoSlopeNorm(vcenter=0.)
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(4, 6))
WxWxT = tf.matmul(Wx, Wx, transpose_b=True)
WxWxT_true = tf.matmul(Wx_true, Wx_true, transpose_b=True)
sns.heatmap(WxWxT, ax=axes[0, 0], cmap='PuOr', norm=divnorm)
sns.heatmap(WxWxT_true, ax=axes[0, 1], cmap='PuOr', norm=divnorm)
plt.show()

# Examine matrix parameters
divnorm=colors.TwoSlopeNorm(vmin=-5., vcenter=0., vmax=10)
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(4, 6))
sns.heatmap(Bx_true, ax=axes[0, 0], cmap='PuOr', norm=divnorm)
sns.heatmap(Bx, ax=axes[0, 1], cmap='PuOr', norm=divnorm)
sns.heatmap(Wx_true, ax=axes[1, 0], cmap='PuOr', norm=divnorm)
sns.heatmap(Wx, ax=axes[1, 1], cmap='PuOr', norm=divnorm)
sns.heatmap(Wy_true, ax=axes[2, 0], cmap='PuOr', norm=divnorm)
sns.heatmap(Wy, ax=axes[2, 1], cmap='PuOr', norm=divnorm)
plt.show()

# Examine vector parameters
fig, axes = plt.subplots(nrows=2, figsize=(12, 6))
sns.heatmap(tf.stack([mu_x_true, mu_x]), ax=axes[0], cmap='PuOr', norm=divnorm)
sns.heatmap(tf.stack([mu_y_true, mu_y]), ax=axes[1], cmap='PuOr', norm=divnorm)
axes[0].set_title('mu_x')
axes[1].set_title('mu_y')
plt.show()
