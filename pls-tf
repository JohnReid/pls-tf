#!/usr/bin/env python

"""Implementation of partial least squares model (section 12.5.2 in Kevin Murphy's PML 2012 book)."""

from pathlib import Path
import io
import math
import datetime
import numpy as np
import tensorflow as tf
from tensorflow import keras
import tensorflow_probability as tfp
import matplotlib.pyplot as plt
from matplotlib import colors
import seaborn as sns
import pls
import importlib
importlib.reload(pls)
la = tf.linalg
tf.__version__
tfp.__version__
tf.random.set_seed(37)
_std_normal = tfp.distributions.Normal(loc=0, scale=1.)
plt.ion()
plt.close('all')
divnorm = colors.TwoSlopeNorm(vmin=-5., vcenter=0., vmax=10)


def WyWxBx_figure():
    Uhat = pls.polar_decomposition(tf.transpose(pls_true.Wx), tf.transpose(model.Wx))
    tf.transpose(pls_true.Wx) - tf.matmul(Uhat, model.Wx, transpose_b=True)
    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 6))
    sns.heatmap(pls_true.Wx, ax=axes[1, 0], cmap='PuOr', norm=divnorm)
    sns.heatmap(tf.matmul(model.Wx, Uhat, transpose_b=True), ax=axes[1, 1], cmap='PuOr', norm=divnorm)
    return fig


def means_figure():
    """Figure comparing estimated means to true means."""
    fig, axes = plt.subplots(nrows=2, figsize=(12, 6))
    sns.heatmap(tf.stack([pls_true.mu_x, model.mu_x]), ax=axes[0], cmap='PuOr', norm=divnorm)
    sns.heatmap(tf.stack([pls_true.mu_y, model.mu_y]), ax=axes[1], cmap='PuOr', norm=divnorm)
    axes[0].set_title('mu_x')
    axes[1].set_title('mu_y')
    return fig


def singular_value_figure():
    """Plot the singular values of each of Wy, Wx, Bx both for the true values and the estimated values."""

    Wy_singular_values = tf.stack([la.svd(model.Wy, compute_uv=False), la.svd(pls_true.Wy, compute_uv=False)])
    Wx_singular_values = tf.stack([la.svd(model.Wx, compute_uv=False), la.svd(pls_true.Wx, compute_uv=False)])
    Bx_singular_values = tf.stack([la.svd(model.Bx, compute_uv=False), la.svd(pls_true.Bx, compute_uv=False)])

    fig, axes = plt.subplots(nrows=3, figsize=(12, 6))
    sns.heatmap(Wy_singular_values, ax=axes[0], cmap='PuOr', norm=divnorm)
    sns.heatmap(Wx_singular_values, ax=axes[1], cmap='PuOr', norm=divnorm)
    sns.heatmap(Bx_singular_values, ax=axes[2], cmap='PuOr', norm=divnorm)
    for ax in axes:
        ax.set_yticks([.5, 1.5])
        ax.set_yticklabels(['Estimated', 'True'])
    axes[0].set_title('Wy')
    axes[1].set_title('Wx')
    axes[2].set_title('Bx')
    return fig


def plot_to_image(figure):
    """Converts the matplotlib plot specified by 'figure' to a PNG image and
    returns it."""
    # Save the plot to a PNG in memory.
    buf = io.BytesIO()
    plt.savefig(buf, format='png')
    buf.seek(0)
    # Convert PNG buffer to TF image
    image = tf.image.decode_png(buf.getvalue(), channels=4)
    # Add the batch dimension
    image = tf.expand_dims(image, 0)
    return image


def log_figure(name, fig, iteration):
    tf.summary.image(name, plot_to_image(fig), step=iteration)
    plt.close(fig)


# Sample ground truth
N = 100
pls_true = pls.PLS.random_init(dx=7, dy=1, dzs=3, dzx=5)
zs_true, zx_true = pls_true.sample_z(N=N)
y_true = pls_true.p_y_given_z(zs_true, zx_true).sample()
x_true = pls_true.p_x_given_z(zs_true, zx_true).sample()
assert zs_true.shape == (N, pls_true.dzs)
assert zx_true.shape == (N, pls_true.dzx)
assert y_true.shape == (N, pls_true.dy)
assert x_true.shape == (N, pls_true.dx)

# Check distribution of ground truth
tf.math.reduce_std(zs_true, axis=0)
tf.math.reduce_std(zx_true, axis=0)
tf.math.reduce_std(y_true, axis=0)
tf.math.reduce_std(x_true, axis=0)

# Parameterise
#
model = pls.PLS(
    # Means
    mu_x = tf.Variable(tf.reduce_mean(x_true, axis=0)),
    mu_y = tf.Variable(tf.reduce_mean(y_true, axis=0)),
    # Data means
    # Maps from latent spaces
    Wy = tf.Variable(_std_normal.sample([pls_true.dy, pls_true.dzs])),
    Wx = tf.Variable(_std_normal.sample([pls_true.dx, pls_true.dzs])),
    Bx = tf.Variable(_std_normal.sample([pls_true.dx, pls_true.dzx])),
    # Noise
    # sigma = tf.Variable(tf.abs(_std_normal.sample([1])), name='sigma')
    sigma = tf.Variable(pls_true.sigma, name='sigma'))
# initialise based on cross-covariance
# Keep all parameters together
trainable_params = [
    # model.sigma,
    model.Bx,
    model.Wx,
    model.Wy,
    model.mu_x,
    model.mu_y]

# Training loop
#
# Instantiate an optimizer.
initial_learning_rate = 1e-2
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=100,
    decay_rate=0.96,
    staircase=True)

# Metrics
# train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)
# train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')

current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
log_dir = Path('logs') / current_time
summary_writer = tf.summary.create_file_writer(str(log_dir))



optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)
iterations = 50
for iteration in range(iterations):
    # Open a GradientTape to record the operations run
    # during the forward pass, which enables auto-differentiation.
    with tf.GradientTape() as tape:

        # Likelihood of y
        lp = model.p_y_given_x(x_true).log_prob(y_true)
        lp_sum = tf.reduce_sum(lp)

        # Parameter priors
        sigma_prior = tf.squeeze(tfp.distributions.LogNormal(loc=0, scale=.25).log_prob(model.sigma))

        # Loss
        joint = lp_sum + sigma_prior
        loss = - joint

    # Use the gradient tape to automatically retrieve
    # the gradients of the trainable variables with respect to the loss.
    grads = tape.gradient(loss, trainable_params)

    # Run one step of gradient descent by updating
    # the value of the variables to minimize the loss.
    optimizer.apply_gradients(zip(grads, trainable_params))

    print("Training joint probability at iteration %d: %.4f" % (iteration, float(joint)))
    with summary_writer.as_default():
        tf.summary.scalar('joint/total', joint, step=iteration)
        tf.summary.scalar('joint/lp_sum', lp_sum, step=iteration)
        tf.summary.scalar('joint/sigma_prior', sigma_prior, step=iteration)
        log_figure('Transforms', WyWxBx_figure(), iteration)
        log_figure('Transform singular values', singular_value_figure(), iteration)
        log_figure('Means', means_figure(), iteration)
        summary_writer.flush()

# Examine matrix parameters
fig, axes = plt.subplots(ncols=3, figsize=(12, 4))
sns.heatmap(tf.matmul(pls_true.Bx, model.Bx, transpose_a=True), ax=axes[0], cmap='PuOr', norm=divnorm)
sns.heatmap(tf.matmul(pls_true.Wx, model.Wx, transpose_a=True), ax=axes[1], cmap='PuOr', norm=divnorm)
sns.heatmap(tf.matmul(pls_true.Wy, model.Wy, transpose_a=True), ax=axes[2], cmap='PuOr', norm=divnorm)
axes[0].set_title('B_x')
axes[1].set_title('W_x')
axes[2].set_title('W_y')
plt.show()

# Examine matrix parameters
divnorm=colors.TwoSlopeNorm(vcenter=0.)
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(4, 6))
WxWxT = tf.matmul(model.Wx, model.Wx, transpose_b=True)
WxWxT_true = tf.matmul(pls_true.Wx, pls_true.Wx, transpose_b=True)
sns.heatmap(WxWxT, ax=axes[0, 0], cmap='PuOr', norm=divnorm)
sns.heatmap(WxWxT_true, ax=axes[0, 1], cmap='PuOr', norm=divnorm)
plt.show()

# Examine matrix parameters
divnorm=colors.TwoSlopeNorm(vmin=-5., vcenter=0., vmax=10)
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(4, 6))
sns.heatmap(pls_true.Bx, ax=axes[0, 0], cmap='PuOr', norm=divnorm)
sns.heatmap(model.Bx, ax=axes[0, 1], cmap='PuOr', norm=divnorm)
sns.heatmap(pls_true.Wx, ax=axes[1, 0], cmap='PuOr', norm=divnorm)
sns.heatmap(model.Wx, ax=axes[1, 1], cmap='PuOr', norm=divnorm)
sns.heatmap(pls_true.Wy, ax=axes[2, 0], cmap='PuOr', norm=divnorm)
sns.heatmap(model.Wy, ax=axes[2, 1], cmap='PuOr', norm=divnorm)
plt.show()
