#!/usr/bin/env python

"""Implementation of partial least squares model (section 12.5.2 in Kevin Murphy's PML 2012 book)."""

from functools import partial
from pathlib import Path
import argparse
import io
import datetime
import tensorflow as tf
from tensorflow import keras
import tensorflow_probability as tfp
import matplotlib.pyplot as plt
from matplotlib import colors
import seaborn as sns
from bluemyrtle import pls
import importlib
importlib.reload(pls)
la = tf.linalg
tf.__version__
tfp.__version__
tf.random.set_seed(37)
plt.ion()
plt.close('all')
output_dir = Path('output')
output_dir.mkdir(exist_ok=True)
divnorm = colors.TwoSlopeNorm(vmin=-5., vcenter=0., vmax=10)
frobenius = partial(tf.norm, ord='fro', axis=(-2, -1))

parser = argparse.ArgumentParser(description='Fit partial least squares.')
parser.add_argument('--N', type=int, default=100, help='number of samples')
parser.add_argument('--iterations', type=int, default=50, help='number of fitting iterations')
parser.add_argument('--learning-rate', type=float, default=1e-1, help='learning rate for optimiser')
parser.add_argument('--dy', type=int, default=11, help='dimensionality of y')
parser.add_argument('--dx', type=int, default=19, help='dimensionality of x')
parser.add_argument('--dzs', type=int, default=3, help='dimensionality of shared latent space, zs')
parser.add_argument('--dzx', type=int, default=5, help='dimensionality of latent space for x, zx')
parser.add_argument('--init-W', default='SVD', help='initialisation method for Wy and Wx parameters')
args = parser.parse_args()
print(args)


def transformed_parameters():
    Us = model.Us_alignment(pls_true.Wy, pls_true.Wx)
    return (
        la.matmul(pls_true.Wy, Us),
        la.matmul(pls_true.Wx, Us),
        pls.diff_transforms_modulo_orthonormal(pls_true.Bx, model.Bx))


def WyWxBx_figure():
    divnorm = colors.TwoSlopeNorm(vcenter=0)
    Wy_trans, Wx_trans, Bx_trans = transformed_parameters()
    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 6))
    sns.heatmap(model.Wy - Wy_trans, ax=axes[0], cmap='PuOr', norm=divnorm)
    sns.heatmap(model.Wx - Wx_trans, ax=axes[1], cmap='PuOr', norm=divnorm)
    sns.heatmap(model.Bx - Bx_trans, ax=axes[2], cmap='PuOr', norm=divnorm)
    axes[0].set_title('W_y')
    axes[1].set_title('W_x')
    axes[2].set_title('B_x')
    return fig


def means_figure():
    """Figure comparing estimated means to true means."""
    fig, axes = plt.subplots(nrows=2, figsize=(12, 6))
    sns.heatmap(tf.stack([pls_true.mu_x, model.mu_x]), ax=axes[0], cmap='PuOr', norm=divnorm)
    sns.heatmap(tf.stack([pls_true.mu_y, model.mu_y]), ax=axes[1], cmap='PuOr', norm=divnorm)
    axes[0].set_title('mu_x')
    axes[1].set_title('mu_y')
    return fig


def singular_value_figure():
    """Plot the singular values of each of Wy, Wx, Bx both for the true values and the estimated values."""

    Wy_singular_values = tf.stack([la.svd(model.Wy, compute_uv=False), la.svd(pls_true.Wy, compute_uv=False)])
    Wx_singular_values = tf.stack([la.svd(model.Wx, compute_uv=False), la.svd(pls_true.Wx, compute_uv=False)])
    Bx_singular_values = tf.stack([la.svd(model.Bx, compute_uv=False), la.svd(pls_true.Bx, compute_uv=False)])

    fig, axes = plt.subplots(nrows=3, figsize=(12, 6))
    sns.heatmap(Wy_singular_values, ax=axes[0], cmap='PuOr', norm=divnorm)
    sns.heatmap(Wx_singular_values, ax=axes[1], cmap='PuOr', norm=divnorm)
    sns.heatmap(Bx_singular_values, ax=axes[2], cmap='PuOr', norm=divnorm)
    for ax in axes:
        ax.set_yticks([.5, 1.5])
        ax.set_yticklabels(['Estimated', 'True'])
    axes[0].set_title('Wy')
    axes[1].set_title('Wx')
    axes[2].set_title('Bx')
    return fig


def plot_to_image(figure):
    """Converts the matplotlib plot specified by 'figure' to a PNG image and
    returns it."""
    # Save the plot to a PNG in memory.
    buf = io.BytesIO()
    plt.savefig(buf, format='png')
    buf.seek(0)
    # Convert PNG buffer to TF image
    image = tf.image.decode_png(buf.getvalue(), channels=4)
    # Add the batch dimension
    image = tf.expand_dims(image, 0)
    return image


def log_figure(name, fig, iteration):
    tf.summary.image(name, plot_to_image(fig), step=iteration)
    plt.close(fig)


# Sample ground truth
pls_true = pls.PLS.random_init(dx=args.dx, dy=args.dy, dzs=args.dzs, dzx=args.dzx)
zs_true, zx_true = pls_true.sample_z(N=args.N)
y_true = pls_true.p_y_given_z(zs_true, zx_true).sample()
x_true = pls_true.p_x_given_z(zs_true, zx_true).sample()
assert zs_true.shape == (args.N, pls_true.dzs)
assert zx_true.shape == (args.N, pls_true.dzx)
assert y_true.shape == (args.N, pls_true.dy)
assert x_true.shape == (args.N, pls_true.dx)

# Check distribution of ground truth
tf.math.reduce_std(zs_true, axis=0)
tf.math.reduce_std(zx_true, axis=0)
tf.math.reduce_std(y_true, axis=0)
tf.math.reduce_std(x_true, axis=0)

# Parameterise
#
parameters = pls_true.parameters
trainable_parameters = dict(
    #
    # Means
    mu_x=tf.Variable(tf.reduce_mean(x_true, axis=0)),
    mu_y=tf.Variable(tf.reduce_mean(y_true, axis=0)),
    #
    # Maps from latent spaces
    Wy=tf.Variable(pls._std_normal.sample([pls_true.dy, pls_true.dzs])),
    Wx=tf.Variable(pls._std_normal.sample([pls_true.dx, pls_true.dzs])),
    Bx=tf.Variable(pls._std_normal.sample([pls_true.dx, pls_true.dzx])),
    #
    # Noise
    sigma=tf.Variable(tfp.distributions.LogNormal(loc=0, scale=1).sample(), name='sigma'),
)
if 'SVD' == args.init_W:
    # Can use SVD to initialise W_y and W_x
    cov_y_x = la.matmul(y_true, x_true, transpose_a=True) / (args.N - 1)
    s, u, v = la.svd(cov_y_x)
    sqrt_s = la.diag(tf.sqrt(s))
    trainable_parameters['Wy'] = tf.Variable(tf.matmul(u, sqrt_s)[:, :pls_true.dzs])
    trainable_parameters['Wx'] = tf.Variable(tf.matmul(v, sqrt_s)[:, :pls_true.dzs])
parameters.update(**trainable_parameters)
model = pls.PLS(**parameters)

current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
log_dir = Path('logs') / current_time
summary_writer = tf.summary.create_file_writer(str(log_dir))

optimizer = keras.optimizers.Adam(learning_rate=args.learning_rate)
for iteration in range(args.iterations):
    # Open a GradientTape to record the operations run
    # during the forward pass, which enables auto-differentiation.
    with tf.GradientTape() as tape:

        # Likelihood of y
        lp = model.p_xy().log_prob(tf.concat((y_true, x_true), axis=-1))
        lp_sum = tf.reduce_sum(lp)

        # Parameter priors
        sigma_prior = tf.squeeze(tfp.distributions.LogNormal(loc=0, scale=.25).log_prob(model.sigma))

        # Loss
        joint = lp_sum + sigma_prior
        loss = - joint

    # Use the gradient tape to automatically retrieve
    # the gradients of the trainable variables with respect to the loss.
    grads = tape.gradient(loss, trainable_parameters.values())

    # Run one step of gradient descent by updating
    # the value of the variables to minimize the loss.
    optimizer.apply_gradients(zip(grads, trainable_parameters.values()))

    print("Training joint probability at iteration %d: %.4f" % (iteration, float(joint)))
    Wy_trans, Wx_trans, Bx_trans = transformed_parameters()
    with summary_writer.as_default():
        tf.summary.scalar('joint/total', joint, step=iteration)
        tf.summary.scalar('joint/lp-sum', lp_sum, step=iteration)
        tf.summary.scalar('joint/sigma-prior', sigma_prior, step=iteration)
        tf.summary.scalar('params/sigma-ratio', model.sigma / pls_true.sigma, step=iteration)
        tf.summary.scalar('params/Wy-frobenius-ratio', frobenius(model.Wy) / frobenius(pls_true.Wy), step=iteration)
        tf.summary.scalar('params/Wx-frobenius-ratio', frobenius(model.Wx) / frobenius(pls_true.Wx), step=iteration)
        tf.summary.scalar('params/Bx-frobenius-ratio', frobenius(model.Bx) / frobenius(pls_true.Bx), step=iteration)
        tf.summary.scalar('params/Wy-frobenius-diff', frobenius(model.Wy - Wy_trans), step=iteration)
        tf.summary.scalar('params/Wx-frobenius-diff', frobenius(model.Wx - Wx_trans), step=iteration)
        tf.summary.scalar('params/Bx-frobenius-diff', frobenius(model.Bx - Bx_trans), step=iteration)
        tf.summary.scalar('params/Wy-max-diff', tf.reduce_max(tf.abs(model.Wy - Wy_trans)), step=iteration)
        tf.summary.scalar('params/Wx-max-diff', tf.reduce_max(tf.abs(model.Wx - Wx_trans)), step=iteration)
        tf.summary.scalar('params/Bx-frobenius-diff', tf.reduce_max(tf.abs(model.Bx - Bx_trans)), step=iteration)
        # log_figure('Transforms', WyWxBx_figure(), iteration)
        # log_figure('Transform singular values', singular_value_figure(), iteration)
        # log_figure('Means', means_figure(), iteration)
        summary_writer.flush()

fig = WyWxBx_figure()
plt.savefig(output_dir / 'WyWxBx.png')
plt.close(fig)

# # Examine matrix parameters
# fig, axes = plt.subplots(ncols=3, figsize=(12, 4))
# sns.heatmap(tf.matmul(pls_true.Bx, model.Bx, transpose_a=True), ax=axes[0], cmap='PuOr', norm=divnorm)
# sns.heatmap(tf.matmul(pls_true.Wx, model.Wx, transpose_a=True), ax=axes[1], cmap='PuOr', norm=divnorm)
# sns.heatmap(tf.matmul(pls_true.Wy, model.Wy, transpose_a=True), ax=axes[2], cmap='PuOr', norm=divnorm)
# axes[0].set_title('B_x')
# axes[1].set_title('W_x')
# axes[2].set_title('W_y')
# # plt.show()

# # Examine matrix parameters
# divnorm = colors.TwoSlopeNorm(vcenter=0.)
# fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(4, 6))
# WxWxT = tf.matmul(model.Wx, model.Wx, transpose_b=True)
# WxWxT_true = tf.matmul(pls_true.Wx, pls_true.Wx, transpose_b=True)
# sns.heatmap(WxWxT, ax=axes[0, 0], cmap='PuOr', norm=divnorm)
# sns.heatmap(WxWxT_true, ax=axes[0, 1], cmap='PuOr', norm=divnorm)
# # plt.show()

# # Examine matrix parameters
# divnorm = colors.TwoSlopeNorm(vmin=-5., vcenter=0., vmax=10)
# fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(4, 6))
# sns.heatmap(pls_true.Bx, ax=axes[0, 0], cmap='PuOr', norm=divnorm)
# sns.heatmap(model.Bx, ax=axes[0, 1], cmap='PuOr', norm=divnorm)
# sns.heatmap(pls_true.Wx, ax=axes[1, 0], cmap='PuOr', norm=divnorm)
# sns.heatmap(model.Wx, ax=axes[1, 1], cmap='PuOr', norm=divnorm)
# sns.heatmap(pls_true.Wy, ax=axes[2, 0], cmap='PuOr', norm=divnorm)
# sns.heatmap(model.Wy, ax=axes[2, 1], cmap='PuOr', norm=divnorm)
# # plt.show()
